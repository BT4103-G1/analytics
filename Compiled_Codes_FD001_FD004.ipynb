{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Compiled-Codes_FD001-FD004.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR-TduwL5m4Z"
      },
      "source": [
        "## 1. Import Relevant Libraries and Reading In of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thz_jf7D2Gky"
      },
      "source": [
        "* 1a. Importing Relevant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2c_xCew5rkx"
      },
      "source": [
        "# Import Required Libaries\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers.core import Activation\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, TimeDistributed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from keras.layers import Bidirectional\n",
        "import math "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucybXgp72VqS"
      },
      "source": [
        "* 1b. Reading in the Data:\n",
        "<br><i> > Training Data\n",
        "<br> > Testing Data\n",
        "<br> > True Labels (RUL) for the Testing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GolPRt8gWD0"
      },
      "source": [
        "> Insert your file path here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgW3AVjvglv3"
      },
      "source": [
        "# FD001 (Dataset 1)\n",
        "train1 = \"/content/train_FD001.txt\"\n",
        "test1 = \"/content/test_FD001.txt\"\n",
        "truth1 = \"/content/RUL_FD001.txt\"\n",
        "\n",
        "# FD002 (Dataset 2)\n",
        "train2 = \"/content/train_FD002.txt\"\n",
        "test2 = \"/content/test_FD002.txt\"\n",
        "truth2 = \"/content/RUL_FD002.txt\"\n",
        "\n",
        "# FD003 (Dataset 3)\n",
        "train3 = \"/content/train_FD003.txt\"\n",
        "test3 = \"/content/test_FD003.txt\"\n",
        "truth3 = \"/content/RUL_FD003.txt\"\n",
        "\n",
        "# FD004 (Dataset 4)\n",
        "train4 = \"/content/train_FD004.txt\"\n",
        "test4 = \"/content/test_FD004.txt\"\n",
        "truth4 = \"/content/RUL_FD004.txt\"\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS_Cu82ShnD-"
      },
      "source": [
        "> Read in data (.txt files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve7h1KnM5zeO"
      },
      "source": [
        "\n",
        "# For FD001\n",
        "# Train Data \n",
        "train1_df = pd.read_csv(train1, sep=\" \", header=None)\n",
        "# Test Data\n",
        "test1_df = pd.read_csv(test1, sep=\" \", header=None)\n",
        "# True RUL Data for the Test Set\n",
        "truth1_df = pd.read_csv(truth1, sep=\" \", header=None)\n",
        "\n",
        "# For FD002\n",
        "# Train Data \n",
        "train2_df = pd.read_csv(train2, sep=\" \", header=None)\n",
        "# Test Data\n",
        "test2_df = pd.read_csv(test2, sep=\" \", header=None)\n",
        "# True RUL Data for the Test Set\n",
        "truth2_df = pd.read_csv(truth2, sep=\" \", header=None)\n",
        "\n",
        "\n",
        "# For FD003\n",
        "# Train Data \n",
        "train3_df = pd.read_csv(train3, sep=\" \", header=None)\n",
        "# Test Data\n",
        "test3_df = pd.read_csv(test3, sep=\" \", header=None)\n",
        "# True RUL Data for the Test Set\n",
        "truth3_df = pd.read_csv(truth3, sep=\" \", header=None)\n",
        "\n",
        "\n",
        "# For FD004\n",
        "# Train Data \n",
        "train4_df = pd.read_csv(train4, sep=\" \", header=None)\n",
        "# Test Data\n",
        "test4_df = pd.read_csv(test4, sep=\" \", header=None)\n",
        "# True RUL Data for the Test Set\n",
        "truth4_df = pd.read_csv(truth4, sep=\" \", header=None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghG_80mg5t-E"
      },
      "source": [
        "* 1c. We set a seed for Reproducibility of Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1iFWcBtAk_D"
      },
      "source": [
        "np.random.seed(1234)  \n",
        "PYTHONHASHSEED = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg53D_xCAnpY"
      },
      "source": [
        "* 1d. We create a path to save the Deep Learning model output "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBNTC4ZchvWZ"
      },
      "source": [
        "> Insert your file path to save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRlMVYCZhzv1"
      },
      "source": [
        "path_save_model = '/content/Output/regression_model.h5'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOEy7RJ3AlcL"
      },
      "source": [
        "# define a path to save model\n",
        "model_path = path_save_model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-QwvxhLA85I"
      },
      "source": [
        "## 2. Data Preprocessing\n",
        "- Dealing with Missing Values: Drop the last 2 columns, because they consist of all null values\n",
        "- Rename columns according to the Engine ID, Cycle, the 3 respective Operating Settings and the 21 Operating Sensors. This aids in better readability and interpretability of the dataset.\n",
        "- Derive RUL labels from the train data. This is because the RUL for the train data is not explicity provided.\n",
        "- MinMax Normalisation to transform our features' values to a value between 0 and 1; For every feature, the minimum value of the feature would be transformed to a value of 0, and the maximum value of the feature would be transformed to a value of 1. \n",
        "- Contextual handling of RUL: Clipping the upper limit of the RUL of aircrafts to mimic a more accurate degradation pattern of the aircraft engine with increasing usage.\n",
        "- Transform the input data into a form (3-Dimensional Form) that can be fed into the deep learning model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swkjjtsw2z62"
      },
      "source": [
        "* 2a. Dealing with Missing Values: Drop the last 2 columns for Training, Testing and True Labels Dataframes\n",
        "> This is because the last 2 columns consists of all null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwohCRTRO3iA"
      },
      "source": [
        "# Drop the last 2 columns for Train, Test and True RUL Dataframes\n",
        "\n",
        "# FD001\n",
        "train1_df.drop(train1_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test1_df.drop(test1_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "truth1_df.drop(truth1_df.columns[[1]], axis=1, inplace=True)\n",
        "\n",
        "# FD002\n",
        "train2_df.drop(train2_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test2_df.drop(test2_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "truth2_df.drop(truth2_df.columns[[1]], axis=1, inplace=True)\n",
        "\n",
        "# FD003\n",
        "train3_df.drop(train3_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test3_df.drop(test3_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "truth3_df.drop(truth3_df.columns[[1]], axis=1, inplace=True)\n",
        "\n",
        "# FD004\n",
        "train4_df.drop(train4_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test4_df.drop(test4_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "truth4_df.drop(truth4_df.columns[[1]], axis=1, inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX8ustEm25Qx"
      },
      "source": [
        "* 2b. Rename Columns in this form for better readability and interpretability of the dataset: \n",
        "<br><i>-> Engine ID\n",
        "<br>-> 3 Operating Settings\n",
        "<br>-> 21 Sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE6dyXzp23G_"
      },
      "source": [
        "# Rename columns into readable forms, namely, by: \n",
        "# [Engine ID, Operational Settings, Sensors]\n",
        "\n",
        "# FD001\n",
        "train1_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "train1_df = train1_df.sort_values(['id','cycle'])\n",
        "\n",
        "\n",
        "test1_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# FD002\n",
        "train2_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "train2_df = train2_df.sort_values(['id','cycle'])\n",
        "\n",
        "\n",
        "test2_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# FD003\n",
        "train3_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "train3_df = train3_df.sort_values(['id','cycle'])\n",
        "\n",
        "\n",
        "test3_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# FD004\n",
        "train4_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "train4_df = train4_df.sort_values(['id','cycle'])\n",
        "\n",
        "\n",
        "test4_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']                                          "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQI_x7RO3OdL"
      },
      "source": [
        "* 2c. Derive RUL labels for the Training Data\n",
        "<br><i> This is because the RUL for the Training Data is not explicity provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uHCLDtXj4XC"
      },
      "source": [
        "Since this dataset is a simulated run-to-failure dataset, the last row for each Engine ID would represent the last cycle of the engine, indicating aircraft failure had already occurred. Hence, since we already know the complete life cycle length of Engines, we employed the following calculations for RUL derivation:\n",
        "> * RUL (of each row/instance of each Engine ID) = Maximum cycle length for each Engine ID - current cycle length (at each row/instance of each Engine ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M60YfXIzP5sH"
      },
      "source": [
        "# Function to derive the RUL labels for the Train Data\n",
        "def extract_rul(data, factor = 0):\n",
        "\n",
        "    # Get the total number of cycles for each unit, i.e. Each Engine ID\n",
        "    # The last row (i.e. maximum value) for each Engine ID would represent the last cycle of the engine\n",
        "    rul = pd.DataFrame(data.groupby('id')['cycle'].max()).reset_index()\n",
        "    rul.columns = ['id', 'max']\n",
        "\n",
        "    # Merge the maximum cycle into the original dataframe\n",
        "    data = data.merge(rul, on=['id'], how='left')\n",
        "\n",
        "    # Actual calculation of RUL (based on the maximum value of the RUL for each Engine)\n",
        "    data['RUL'] = data['max'] - data['cycle']\n",
        "\n",
        "    # Drop the 'max' column, which now does not add any value\n",
        "    data.drop(columns=['max'], axis = 1, inplace = True)\n",
        "  \n",
        "    return data[data['cycle'] > factor]\n",
        "\n",
        "# Apply the function on the training data:\n",
        "# FD001\n",
        "train1_df = extract_rul(train1_df, factor = 0)\n",
        "\n",
        "# FD002\n",
        "train2_df = extract_rul(train2_df, factor = 0)\n",
        "\n",
        "# FD003\n",
        "train3_df = extract_rul(train3_df, factor = 0)\n",
        "\n",
        "# FD004\n",
        "train4_df = extract_rul(train4_df, factor = 0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--EaD7hR3XI_"
      },
      "source": [
        "* 2d. Contextual Handling of RUL: Clipping the maximum RUL to aircraft engines for Training Data \n",
        "> This is to more accurately portray the degradation pattern of aircraft engines after a certain period of usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uP-P1xSRIXw"
      },
      "source": [
        "# Clipping the maximum RUL to aircraft engines (for train data):\n",
        "\n",
        "# FD001\n",
        "train1_df['RUL'] = train1_df['RUL'].clip(upper=125)\n",
        "\n",
        "# FD002\n",
        "train2_df['RUL'] = train2_df['RUL'].clip(upper=125)\n",
        "\n",
        "# FD003\n",
        "train3_df['RUL'] = train3_df['RUL'].clip(upper=125)\n",
        "\n",
        "# FD004\n",
        "train4_df['RUL'] = train4_df['RUL'].clip(upper=125)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAlEFLfR3i4N"
      },
      "source": [
        "* 2e. MinMax Normalisation of Training Data\n",
        "> * Transforms our features' values to a value between 0 and 1; \n",
        "> * The minimum value of the feature would be transformed to a value of 0, and the maximum value of the feature would be transformed to a value of 1. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esDY2KYs3Q8b"
      },
      "source": [
        "# FD001\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for train set\n",
        "train1_df['cycle_norm'] = train1_df['cycle']\n",
        "cols_normalize = train1_df.columns.difference(['id','cycle','RUL'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train1_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train1_df.index)\n",
        "join_df = train1_df[train1_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train1_df = join_df.reindex(columns = train1_df.columns)\n",
        "\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for test set\n",
        "test1_df['cycle_norm'] = test1_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test1_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test1_df.index)\n",
        "test_join_df = test1_df[test1_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test1_df = test_join_df.reindex(columns = test1_df.columns)\n",
        "test1_df = test1_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# FD002\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for train set\n",
        "train2_df['cycle_norm'] = train2_df['cycle']\n",
        "cols_normalize = train2_df.columns.difference(['id','cycle','RUL'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train2_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train2_df.index)\n",
        "join_df = train2_df[train1_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train2_df = join_df.reindex(columns = train2_df.columns)\n",
        "\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for test set\n",
        "test2_df['cycle_norm'] = test2_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test2_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test2_df.index)\n",
        "test_join_df = test2_df[test1_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test2_df = test_join_df.reindex(columns = test2_df.columns)\n",
        "test2_df = test2_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# FD003\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for train set\n",
        "train3_df['cycle_norm'] = train3_df['cycle']\n",
        "cols_normalize = train3_df.columns.difference(['id','cycle','RUL'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train3_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train3_df.index)\n",
        "join_df = train3_df[train1_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train3_df = join_df.reindex(columns = train3_df.columns)\n",
        "\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for test set\n",
        "test3_df['cycle_norm'] = test3_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test3_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test3_df.index)\n",
        "test_join_df = test3_df[test3_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test3_df = test_join_df.reindex(columns = test3_df.columns)\n",
        "test3_df = test3_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# FD004\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for train set\n",
        "train4_df['cycle_norm'] = train4_df['cycle']\n",
        "cols_normalize = train4_df.columns.difference(['id','cycle','RUL'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train4_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train4_df.index)\n",
        "join_df = train4_df[train4_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train4_df = join_df.reindex(columns = train4_df.columns)\n",
        "\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for test set\n",
        "test4_df['cycle_norm'] = test4_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test4_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test4_df.index)\n",
        "test_join_df = test4_df[test1_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test4_df = test_join_df.reindex(columns = test4_df.columns)\n",
        "test4_df = test4_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPs4Mpfj5Y0O"
      },
      "source": [
        "* 2f. Processing of Test Data: add the True RUL Labels (Truth Dataframe) to the Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8wBrqgxSFlh"
      },
      "source": [
        "# Generate labels (the RUL) for the Test Data using the dataset containing the true RUL (Truth Dataframe) for the Test Data.\n",
        "\n",
        "# FD001\n",
        "rul = pd.DataFrame(test1_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "# rename column in Truth Dataframe\n",
        "truth1_df.columns = ['rul_init']\n",
        "# assign the RUL to the Engine IDs by adding a column in Truth Dataframe\n",
        "truth1_df['id'] = truth1_df.index + 1\n",
        "# assign RUL to a newly-named column, max\n",
        "truth1_df['max'] = rul['max'] + truth1_df['rul_init']\n",
        "truth1_df.drop('rul_init', axis=1, inplace=True)\n",
        "\n",
        "# Merge Truth Dataframe with Test Dataframe to put the actual RUL together with the data in Test Dataframe\n",
        "# Merge based on Engine ID\n",
        "test1_df = test1_df.merge(truth1_df, on=['id'], how='left')\n",
        "# Final RUL matched to Engine ID\n",
        "test1_df['RUL'] = test1_df['max'] - test1_df['cycle']\n",
        "\n",
        "# Clipping the maximum RUL to aircraft engines.\n",
        "test1_df['RUL'] = test1_df['RUL'].clip(upper=125)\n",
        "test1_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# FD002\n",
        "rul = pd.DataFrame(test2_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "# rename column in Truth Dataframe\n",
        "truth2_df.columns = ['rul_init']\n",
        "# assign the RUL to the Engine IDs by adding a column in Truth Dataframe\n",
        "truth2_df['id'] = truth2_df.index + 1\n",
        "# assign RUL to a newly-named column, max\n",
        "truth2_df['max'] = rul['max'] + truth2_df['rul_init']\n",
        "truth2_df.drop('rul_init', axis=1, inplace=True)\n",
        "\n",
        "# Merge Truth Dataframe with Test Dataframe to put the actual RUL together with the data in Test Dataframe\n",
        "# Merge based on Engine ID\n",
        "test2_df = test2_df.merge(truth2_df, on=['id'], how='left')\n",
        "# Final RUL matched to Engine ID\n",
        "test2_df['RUL'] = test2_df['max'] - test2_df['cycle']\n",
        "\n",
        "# Clipping the maximum RUL to aircraft engines.\n",
        "test2_df['RUL'] = test2_df['RUL'].clip(upper=125)\n",
        "test2_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# FD003\n",
        "rul = pd.DataFrame(test3_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "# rename column in Truth Dataframe\n",
        "truth3_df.columns = ['rul_init']\n",
        "# assign the RUL to the Engine IDs by adding a column in Truth Dataframe\n",
        "truth3_df['id'] = truth3_df.index + 1\n",
        "# assign RUL to a newly-named column, max\n",
        "truth3_df['max'] = rul['max'] + truth3_df['rul_init']\n",
        "truth3_df.drop('rul_init', axis=1, inplace=True)\n",
        "\n",
        "# Merge Truth Dataframe with Test Dataframe to put the actual RUL together with the data in Test Dataframe\n",
        "# Merge based on Engine ID\n",
        "test3_df = test3_df.merge(truth3_df, on=['id'], how='left')\n",
        "# Final RUL matched to Engine ID\n",
        "test3_df['RUL'] = test3_df['max'] - test3_df['cycle']\n",
        "\n",
        "# Clipping the maximum RUL to aircraft engines.\n",
        "test3_df['RUL'] = test3_df['RUL'].clip(upper=125)\n",
        "test3_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# FD004\n",
        "rul = pd.DataFrame(test4_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "# rename column in Truth Dataframe\n",
        "truth4_df.columns = ['rul_init']\n",
        "# assign the RUL to the Engine IDs by adding a column in Truth Dataframe\n",
        "truth4_df['id'] = truth4_df.index + 1\n",
        "# assign RUL to a newly-named column, max\n",
        "truth4_df['max'] = rul['max'] + truth4_df['rul_init']\n",
        "truth4_df.drop('rul_init', axis=1, inplace=True)\n",
        "\n",
        "# Merge Truth Dataframe with Test Dataframe to put the actual RUL together with the data in Test Dataframe\n",
        "# Merge based on Engine ID\n",
        "test4_df = test4_df.merge(truth4_df, on=['id'], how='left')\n",
        "# Final RUL matched to Engine ID\n",
        "test4_df['RUL'] = test4_df['max'] - test4_df['cycle']\n",
        "\n",
        "# Clipping the maximum RUL to aircraft engines.\n",
        "test4_df['RUL'] = test4_df['RUL'].clip(upper=125)\n",
        "test4_df.drop('max', axis=1, inplace=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl2HMyD65hb9"
      },
      "source": [
        "* 2g: Transform data into a form that can be fed into the Deep Learning Model as input.\n",
        "> The form is: [samples, time steps, features]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg1KaBYi2VB7"
      },
      "source": [
        "# Transforming data into a form that can be fed in to the Deep Learning Model \n",
        "# The Form is a(3 Dimensional Form): (samples, time steps, features) \n",
        "\n",
        "# Assign sequence length of 50 \n",
        "sequence_length = 50\n",
        "# Function to generate the sequence for the data: \n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yye0wym13it6"
      },
      "source": [
        "# Feature columns consists of the Operational Settings and Sensor Columns: \n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kFDhzq5pRPp"
      },
      "source": [
        "## Start Modelling for FD001 (Dataset 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzSWWROpUHX"
      },
      "source": [
        "seq_gen = (list(gen_sequence(train1_df[train1_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train1_df['id'].unique())\n",
        "\n",
        "# Generate a sequence with the gen_sequence function to get the 3-Dimensional Form \n",
        "# Convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6r-9he23t06",
        "outputId": "b3dbe9ba-bef4-4517-b0c2-9072ddb5f3d9"
      },
      "source": [
        "# Function to generate the RUL labels \n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# Generate the labels using the gen_labels function\n",
        "label_gen = [gen_labels(train1_df[train1_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train1_df['id'].unique()]\n",
        "# Convert to numpy array\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15631, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60gq0BSyMFxX"
      },
      "source": [
        "## 3. Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRpsXr_16CT6"
      },
      "source": [
        "* 3a. Defining R2 to be used as an evaluation metric\n",
        "> R2 is a statistical measures which gives an indication of closeness of fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf1ssR9A4Ec2"
      },
      "source": [
        "# Defining R2 to be used as an evaluation metric in the Deep Learning Model Evaluation Metrics\n",
        "def r2_keras(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAwRm9W9UiLB"
      },
      "source": [
        "## Deep Learning Architecture and Topology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVo_bDmL7b3y"
      },
      "source": [
        "* 3b. Deep Learning Architecture consists of:\n",
        "<br> > Sequential Model\n",
        "<br> > Bi-LSTM Layer\n",
        "<br> > LSTM Layer\n",
        "<br> > Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QiNrt2VugfV",
        "outputId": "e647ca56-0b49-4802-87e6-53676f8cb688"
      },
      "source": [
        "# Building the Deep Learning Architecture\n",
        "\n",
        "# Defining the features to be fed into the input of the layers in the Deep Learning Model\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "# Sequential Model\n",
        "model = Sequential()\n",
        "# Add a Bi-LSTM Layer\n",
        "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(sequence_length, nb_features)))\n",
        "# Add a Dropout Layer after the Bi-LSTM Layer to minimize overfitting  \n",
        "model.add(Dropout(0.2))\n",
        "# Add a LSTM Layer\n",
        "model.add(LSTM(units=50,return_sequences=False, activation = 'tanh'))\n",
        "# Add a Dropout Layer after the LSTM Layer to minimize overfitting  \n",
        "model.add(Dropout(0.2))\n",
        "# Dense Layer\n",
        "model.add(Dense(units=nb_out))\n",
        "# Activation Function \n",
        "model.add(Activation(\"linear\"))\n",
        "# Compile model, set metrics for evaluation\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae','mse',r2_keras])\n",
        "# For model observation\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the Deep Learning network on our training data, early stopping is also applied\n",
        "history = model.fit(seq_array, label_array, epochs=100, batch_size=10, validation_split=0.05, verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)])\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 50, 40)            7360      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50, 40)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 50)                18200     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 25,611\n",
            "Trainable params: 25,611\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "1485/1485 - 51s - loss: 3179.2861 - mae: 46.9455 - mse: 3179.2861 - r2_keras: -1.2839e+00 - val_loss: 1735.4110 - val_mae: 36.7932 - val_mse: 1735.4110 - val_r2_keras: -5.5905e+10\n",
            "Epoch 2/100\n",
            "1485/1485 - 16s - loss: 1748.8400 - mae: 36.8465 - mse: 1748.8400 - r2_keras: -1.8020e-01 - val_loss: 1698.9039 - val_mae: 36.3585 - val_mse: 1698.9039 - val_r2_keras: -4.5270e+10\n",
            "Epoch 3/100\n",
            "1485/1485 - 16s - loss: 768.7433 - mae: 22.5291 - mse: 768.7433 - r2_keras: 0.4546 - val_loss: 393.2356 - val_mae: 15.6985 - val_mse: 393.2356 - val_r2_keras: -6.2829e+09\n",
            "Epoch 4/100\n",
            "1485/1485 - 16s - loss: 290.6805 - mae: 13.2472 - mse: 290.6805 - r2_keras: 0.7794 - val_loss: 220.2822 - val_mae: 10.6509 - val_mse: 220.2822 - val_r2_keras: -9.6134e+09\n",
            "Epoch 5/100\n",
            "1485/1485 - 16s - loss: 233.2864 - mae: 11.5669 - mse: 233.2864 - r2_keras: 0.8217 - val_loss: 145.4819 - val_mae: 9.2176 - val_mse: 145.4819 - val_r2_keras: -4.0671e+09\n",
            "Epoch 6/100\n",
            "1485/1485 - 16s - loss: 205.1436 - mae: 10.7539 - mse: 205.1436 - r2_keras: 0.8440 - val_loss: 136.2652 - val_mae: 8.6708 - val_mse: 136.2652 - val_r2_keras: -4.5591e+09\n",
            "Epoch 7/100\n",
            "1485/1485 - 16s - loss: 188.4283 - mae: 10.1690 - mse: 188.4283 - r2_keras: 0.8582 - val_loss: 135.0155 - val_mae: 8.5620 - val_mse: 135.0155 - val_r2_keras: -4.9016e+09\n",
            "Epoch 8/100\n",
            "1485/1485 - 16s - loss: 179.4815 - mae: 9.8775 - mse: 179.4815 - r2_keras: 0.8636 - val_loss: 155.2663 - val_mae: 9.8397 - val_mse: 155.2663 - val_r2_keras: -2.9370e+09\n",
            "Epoch 9/100\n",
            "1485/1485 - 16s - loss: 173.2886 - mae: 9.6632 - mse: 173.2886 - r2_keras: 0.8676 - val_loss: 168.7783 - val_mae: 8.9612 - val_mse: 168.7783 - val_r2_keras: -1.1856e+09\n",
            "Epoch 10/100\n",
            "1485/1485 - 16s - loss: 168.4953 - mae: 9.5068 - mse: 168.4953 - r2_keras: 0.8685 - val_loss: 124.2963 - val_mae: 7.8200 - val_mse: 124.2963 - val_r2_keras: -3.2142e+09\n",
            "Epoch 11/100\n",
            "1485/1485 - 16s - loss: 161.1783 - mae: 9.2805 - mse: 161.1783 - r2_keras: 0.8770 - val_loss: 116.1062 - val_mae: 7.8484 - val_mse: 116.1062 - val_r2_keras: -2.2257e+09\n",
            "Epoch 12/100\n",
            "1485/1485 - 16s - loss: 158.4914 - mae: 9.2226 - mse: 158.4914 - r2_keras: 0.8765 - val_loss: 107.2198 - val_mae: 7.9335 - val_mse: 107.2198 - val_r2_keras: -1.8081e+09\n",
            "Epoch 13/100\n",
            "1485/1485 - 15s - loss: 155.3228 - mae: 9.0748 - mse: 155.3228 - r2_keras: 0.8834 - val_loss: 176.3243 - val_mae: 9.3986 - val_mse: 176.3243 - val_r2_keras: -6.0807e+08\n",
            "Epoch 14/100\n",
            "1485/1485 - 16s - loss: 154.6222 - mae: 9.0416 - mse: 154.6222 - r2_keras: 0.8807 - val_loss: 112.4782 - val_mae: 7.8977 - val_mse: 112.4782 - val_r2_keras: -1.2065e+09\n",
            "Epoch 15/100\n",
            "1485/1485 - 15s - loss: 152.1745 - mae: 8.9602 - mse: 152.1745 - r2_keras: 0.8836 - val_loss: 183.6893 - val_mae: 9.4198 - val_mse: 183.6893 - val_r2_keras: -6.7571e+09\n",
            "Epoch 16/100\n",
            "1485/1485 - 16s - loss: 146.1746 - mae: 8.7672 - mse: 146.1746 - r2_keras: 0.8894 - val_loss: 120.1596 - val_mae: 7.6541 - val_mse: 120.1596 - val_r2_keras: -8.6662e+08\n",
            "Epoch 17/100\n",
            "1485/1485 - 15s - loss: 143.2397 - mae: 8.6800 - mse: 143.2397 - r2_keras: 0.8914 - val_loss: 117.7629 - val_mae: 7.6832 - val_mse: 117.7629 - val_r2_keras: -3.0016e+08\n",
            "Epoch 18/100\n",
            "1485/1485 - 16s - loss: 139.6078 - mae: 8.5667 - mse: 139.6078 - r2_keras: 0.8925 - val_loss: 115.5871 - val_mae: 7.5623 - val_mse: 115.5871 - val_r2_keras: -1.2205e+09\n",
            "Epoch 19/100\n",
            "1485/1485 - 16s - loss: 135.7382 - mae: 8.4448 - mse: 135.7382 - r2_keras: 0.8961 - val_loss: 127.3159 - val_mae: 7.6765 - val_mse: 127.3159 - val_r2_keras: -2.4766e+09\n",
            "Epoch 20/100\n",
            "1485/1485 - 16s - loss: 136.7512 - mae: 8.4543 - mse: 136.7512 - r2_keras: 0.8931 - val_loss: 115.1612 - val_mae: 7.5716 - val_mse: 115.1612 - val_r2_keras: -1.7108e+09\n",
            "Epoch 21/100\n",
            "1485/1485 - 16s - loss: 134.9479 - mae: 8.3528 - mse: 134.9479 - r2_keras: 0.8908 - val_loss: 125.4295 - val_mae: 8.2868 - val_mse: 125.4295 - val_r2_keras: -3.3888e+09\n",
            "Epoch 22/100\n",
            "1485/1485 - 16s - loss: 127.8951 - mae: 8.1537 - mse: 127.8951 - r2_keras: 0.9016 - val_loss: 126.4916 - val_mae: 7.7316 - val_mse: 126.4916 - val_r2_keras: -2.1331e+09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIdo_0JC8Ds8"
      },
      "source": [
        "* 3c. Training Data Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxCoSBIU6KIS",
        "outputId": "60054ab4-8f4f-4b20-9c86-c4c284fcc3f4"
      },
      "source": [
        "# Metrics from fitting the model on our Training Data\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('\\nMAE: {}'.format(scores[1]))\n",
        "print('\\nMSE: {}'.format(scores[2]))\n",
        "print('\\nR^2: {}'.format(scores[3]))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 7ms/step - loss: 79.8803 - mae: 6.0701 - mse: 79.8803 - r2_keras: 0.9343\n",
            "\n",
            "MAE: 6.070121765136719\n",
            "\n",
            "MSE: 79.88032531738281\n",
            "\n",
            "R^2: 0.9343398809432983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa18svH-8H4X"
      },
      "source": [
        "* 3d. Preparation of Test Data to feed into our model for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdTs1FAP6UXP"
      },
      "source": [
        "# Preparing Test Data to be fed into our model for evaluation\n",
        "seq_array_test_last = [test1_df[test1_df['id']==id][sequence_cols].values[-sequence_length:] \n",
        "                       for id in test1_df['id'].unique() if len(test1_df[test1_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_1VG4pA7ISQ"
      },
      "source": [
        "# Preparing Test Data to be fed into our model for evaluation\n",
        "y_mask = [len(test1_df[test1_df['id']==id]) >= sequence_length for id in test1_df['id'].unique()]\n",
        "label_array_test_last = test1_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsIoTFZ08PnA"
      },
      "source": [
        "* 3e. Testing Data Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkPBwLRy7Jat",
        "outputId": "9ab4ea09-1d4b-42d1-93d2-eb4034e6b218"
      },
      "source": [
        "# Test Data Metrics after running our model\n",
        "scores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose = 1, batch_size = 200)\n",
        "print('\\nMAE: {}'.format(scores_test[1]))\n",
        "print('\\nMSE: {}'.format(scores_test[2]))\n",
        "print('\\nRMSE:')\n",
        "print(math.sqrt(scores_test[2]))\n",
        "print('\\nR2: {}'.format(scores_test[3]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 24ms/step - loss: 149.9457 - mae: 8.7375 - mse: 149.9457 - r2_keras: 0.9060\n",
            "\n",
            "MAE: 8.737482070922852\n",
            "\n",
            "MSE: 149.94573974609375\n",
            "\n",
            "RMSE:\n",
            "12.245233347964168\n",
            "\n",
            "R2: 0.9060359001159668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xig-6TmG8U4i"
      },
      "source": [
        "# Predicted Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALDR7gbrI6Ve",
        "outputId": "28aa5063-4d87-402d-90e3-f0ef39e5290d"
      },
      "source": [
        "# To obtain our model's prediction on the Test Data\n",
        "scores_test = model.predict(seq_array_test_last)\n",
        "scores_test"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 54.066944 ],\n",
              "       [ 74.4804   ],\n",
              "       [ 85.37231  ],\n",
              "       [116.49924  ],\n",
              "       [120.04634  ],\n",
              "       [ 88.8193   ],\n",
              "       [120.0387   ],\n",
              "       [ 93.30019  ],\n",
              "       [ 85.20327  ],\n",
              "       [111.70329  ],\n",
              "       [104.94364  ],\n",
              "       [115.537834 ],\n",
              "       [ 97.048454 ],\n",
              "       [ 52.24236  ],\n",
              "       [ 27.858746 ],\n",
              "       [ 80.0188   ],\n",
              "       [ 11.161173 ],\n",
              "       [ 61.223953 ],\n",
              "       [120.7806   ],\n",
              "       [ 15.55234  ],\n",
              "       [119.690636 ],\n",
              "       [ 70.82783  ],\n",
              "       [113.07492  ],\n",
              "       [108.31373  ],\n",
              "       [ 84.39836  ],\n",
              "       [  3.6492057],\n",
              "       [ 43.710194 ],\n",
              "       [120.157646 ],\n",
              "       [  5.284668 ],\n",
              "       [  8.797317 ],\n",
              "       [ 20.59034  ],\n",
              "       [ 21.422215 ],\n",
              "       [ 64.04078  ],\n",
              "       [ 23.351986 ],\n",
              "       [ 21.065279 ],\n",
              "       [  9.059647 ],\n",
              "       [ 68.48787  ],\n",
              "       [118.648186 ],\n",
              "       [ 90.32939  ],\n",
              "       [ 39.24406  ],\n",
              "       [122.51007  ],\n",
              "       [115.14933  ],\n",
              "       [ 20.756863 ],\n",
              "       [ 69.4942   ],\n",
              "       [ 98.19718  ],\n",
              "       [ 26.233906 ],\n",
              "       [ 30.761774 ],\n",
              "       [121.85482  ],\n",
              "       [124.248535 ],\n",
              "       [ 11.705321 ],\n",
              "       [106.00929  ],\n",
              "       [ 37.97789  ],\n",
              "       [102.045876 ],\n",
              "       [112.57393  ],\n",
              "       [ 19.76249  ],\n",
              "       [ 49.046707 ],\n",
              "       [ 61.12174  ],\n",
              "       [ 24.906116 ],\n",
              "       [122.46236  ],\n",
              "       [ 12.084015 ],\n",
              "       [121.89621  ],\n",
              "       [  5.722391 ],\n",
              "       [119.688126 ],\n",
              "       [ 84.7195   ],\n",
              "       [101.840416 ],\n",
              "       [ 51.863876 ],\n",
              "       [ 99.99337  ],\n",
              "       [115.64665  ],\n",
              "       [100.07192  ],\n",
              "       [  4.6523404],\n",
              "       [ 25.197338 ],\n",
              "       [124.849785 ],\n",
              "       [ 80.21988  ],\n",
              "       [ 91.17603  ],\n",
              "       [  4.9771643],\n",
              "       [  7.7025366],\n",
              "       [120.56385  ],\n",
              "       [ 66.70891  ],\n",
              "       [ 93.106834 ],\n",
              "       [121.54304  ],\n",
              "       [113.41859  ],\n",
              "       [124.94863  ],\n",
              "       [ 20.276987 ],\n",
              "       [ 34.63695  ],\n",
              "       [ 18.690836 ],\n",
              "       [ 57.477215 ],\n",
              "       [ 57.95247  ],\n",
              "       [108.0534   ],\n",
              "       [118.08646  ],\n",
              "       [ 78.99661  ],\n",
              "       [ 69.60584  ],\n",
              "       [122.708755 ],\n",
              "       [ 17.75512  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQUU577utbfq"
      },
      "source": [
        "## SHAP Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q8J9nzIteba",
        "outputId": "3ec7a4d4-1ba3-46a3-cb36-0c953a941e0f"
      },
      "source": [
        "%pip install shap\n",
        "import shap"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.7/dist-packages (0.39.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.7/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (54.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE2cZ7uh1E3m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPcjLN7ix9KU"
      },
      "source": [
        "## Start Modelling for FD002 (Dataset 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq8_l-b4yCHT"
      },
      "source": [
        "seq_gen = (list(gen_sequence(train2_df[train2_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train2_df['id'].unique())\n",
        "\n",
        "# Generate a sequence with the gen_sequence function to get the 3-Dimensional Form \n",
        "# Convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqDmYgo_yFJj",
        "outputId": "9faa249a-32f1-417a-b082-e59c146ff593"
      },
      "source": [
        "# Function to generate the RUL labels \n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# Generate the labels using the gen_labels function\n",
        "label_gen = [gen_labels(train2_df[train2_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train2_df['id'].unique()]\n",
        "# Convert to numpy array\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40759, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuqHY1rcyam4"
      },
      "source": [
        "## 3. Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sloEQgRoybpc"
      },
      "source": [
        "* 3a. Defining R2 to be used as an evaluation metric\n",
        "> R2 is a statistical measures which gives an indication of closeness of fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXRGduoVydzG"
      },
      "source": [
        "# Defining R2 to be used as an evaluation metric in the Deep Learning Model Evaluation Metrics\n",
        "def r2_keras(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1337CSC9yg2l"
      },
      "source": [
        "## Deep Learning Architecture and Topology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD5dNcOiyilF"
      },
      "source": [
        "* 3b. Deep Learning Architecture consists of:\n",
        "<br> > Sequential Model\n",
        "<br> > Bi-LSTM Layer\n",
        "<br> > LSTM Layer\n",
        "<br> > Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Juj5PIW-ye4y",
        "outputId": "3700ca3f-20b0-4dbf-c799-5981737afb53"
      },
      "source": [
        "# Building the Deep Learning Architecture\n",
        "\n",
        "# Defining the features to be fed into the input of the layers in the Deep Learning Model\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "# Sequential Model\n",
        "model = Sequential()\n",
        "# Add a Bi-LSTM Layer\n",
        "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(sequence_length, nb_features)))\n",
        "# Add a Dropout Layer after the Bi-LSTM Layer to minimize overfitting  \n",
        "model.add(Dropout(0.2))\n",
        "# Add a LSTM Layer\n",
        "model.add(LSTM(units=50,return_sequences=False, activation = 'tanh'))\n",
        "# Add a Dropout Layer after the LSTM Layer to minimize overfitting  \n",
        "model.add(Dropout(0.2))\n",
        "# Dense Layer\n",
        "model.add(Dense(units=nb_out))\n",
        "# Activation Function \n",
        "model.add(Activation(\"linear\"))\n",
        "# Compile model, set metrics for evaluation\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae','mse',r2_keras])\n",
        "# For model observation\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the Deep Learning network on our training data, early stopping is also applied\n",
        "history = model.fit(seq_array, label_array, epochs=100, batch_size=10, validation_split=0.05, verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_2 (Bidirection (None, 50, 40)            7360      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 50, 40)            0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 50)                18200     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 25,611\n",
            "Trainable params: 25,611\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "3873/3873 - 48s - loss: 1996.8812 - mae: 36.8156 - mse: 1996.8812 - r2_keras: -1.2994e+05 - val_loss: 1612.4198 - val_mae: 32.6678 - val_mse: 1612.4198 - val_r2_keras: -7.4370e+10\n",
            "Epoch 2/100\n",
            "3873/3873 - 41s - loss: 828.8021 - mae: 23.2446 - mse: 828.8021 - r2_keras: -2.7407e+06 - val_loss: 1342.4004 - val_mae: 29.4478 - val_mse: 1342.4004 - val_r2_keras: -7.7522e+10\n",
            "Epoch 3/100\n",
            "3873/3873 - 41s - loss: 700.0065 - mae: 21.0095 - mse: 700.0065 - r2_keras: -1.3372e+05 - val_loss: 698.2231 - val_mae: 22.6075 - val_mse: 698.2231 - val_r2_keras: -1.7726e+10\n",
            "Epoch 4/100\n",
            "3873/3873 - 41s - loss: 633.4789 - mae: 19.8855 - mse: 633.4789 - r2_keras: -3.2263e+06 - val_loss: 1632.4691 - val_mae: 33.0829 - val_mse: 1632.4691 - val_r2_keras: -8.5474e+09\n",
            "Epoch 5/100\n",
            "3873/3873 - 41s - loss: 584.2935 - mae: 19.0188 - mse: 584.2935 - r2_keras: -4.1607e+05 - val_loss: 541.8590 - val_mae: 19.1750 - val_mse: 541.8590 - val_r2_keras: -2.4983e+10\n",
            "Epoch 6/100\n",
            "3873/3873 - 41s - loss: 536.9842 - mae: 18.1480 - mse: 536.9842 - r2_keras: -2.6367e+02 - val_loss: 770.3983 - val_mae: 22.0577 - val_mse: 770.3983 - val_r2_keras: -3.6757e+10\n",
            "Epoch 7/100\n",
            "3873/3873 - 41s - loss: 485.3052 - mae: 17.1495 - mse: 485.3052 - r2_keras: -1.6939e+04 - val_loss: 413.2561 - val_mae: 17.0785 - val_mse: 413.2561 - val_r2_keras: -1.6800e+10\n",
            "Epoch 8/100\n",
            "3873/3873 - 41s - loss: 418.1933 - mae: 15.7535 - mse: 418.1933 - r2_keras: -3.4263e+04 - val_loss: 467.7431 - val_mae: 17.6773 - val_mse: 467.7431 - val_r2_keras: -2.0890e+10\n",
            "Epoch 9/100\n",
            "3873/3873 - 41s - loss: 380.6815 - mae: 14.9126 - mse: 380.6815 - r2_keras: -5.9705e+05 - val_loss: 272.3103 - val_mae: 12.6682 - val_mse: 272.3103 - val_r2_keras: -1.0169e+10\n",
            "Epoch 10/100\n",
            "3873/3873 - 41s - loss: 352.9196 - mae: 14.3080 - mse: 352.9196 - r2_keras: -1.4982e+04 - val_loss: 596.1693 - val_mae: 19.3609 - val_mse: 596.1693 - val_r2_keras: -3.4012e+10\n",
            "Epoch 11/100\n",
            "3873/3873 - 41s - loss: 331.6659 - mae: 13.8013 - mse: 331.6659 - r2_keras: -1.5327e+06 - val_loss: 269.2140 - val_mae: 12.5996 - val_mse: 269.2140 - val_r2_keras: -1.3286e+10\n",
            "Epoch 12/100\n",
            "3873/3873 - 41s - loss: 315.9150 - mae: 13.4312 - mse: 315.9150 - r2_keras: -5.2718e+04 - val_loss: 220.8387 - val_mae: 11.3357 - val_mse: 220.8387 - val_r2_keras: -5.8400e+09\n",
            "Epoch 13/100\n",
            "3873/3873 - 41s - loss: 304.8489 - mae: 13.1874 - mse: 304.8489 - r2_keras: -1.3498e+06 - val_loss: 863.7168 - val_mae: 23.1067 - val_mse: 863.7168 - val_r2_keras: -4.6558e+10\n",
            "Epoch 14/100\n",
            "3873/3873 - 41s - loss: 293.5685 - mae: 12.9276 - mse: 293.5685 - r2_keras: -4.1560e+04 - val_loss: 209.3626 - val_mae: 10.8395 - val_mse: 209.3626 - val_r2_keras: -8.5443e+09\n",
            "Epoch 15/100\n",
            "3873/3873 - 41s - loss: 284.2842 - mae: 12.6639 - mse: 284.2842 - r2_keras: -2.3510e+06 - val_loss: 740.3382 - val_mae: 21.1206 - val_mse: 740.3382 - val_r2_keras: -3.4923e+10\n",
            "Epoch 16/100\n",
            "3873/3873 - 41s - loss: 278.5118 - mae: 12.5266 - mse: 278.5118 - r2_keras: -1.7483e+03 - val_loss: 236.6060 - val_mae: 11.9151 - val_mse: 236.6060 - val_r2_keras: -9.4803e+09\n",
            "Epoch 17/100\n",
            "3873/3873 - 41s - loss: 271.9563 - mae: 12.3661 - mse: 271.9563 - r2_keras: -3.2411e+04 - val_loss: 263.6664 - val_mae: 12.6847 - val_mse: 263.6664 - val_r2_keras: -2.3415e+09\n",
            "Epoch 18/100\n",
            "3873/3873 - 41s - loss: 267.4696 - mae: 12.2645 - mse: 267.4696 - r2_keras: -2.2385e+04 - val_loss: 318.7177 - val_mae: 12.1492 - val_mse: 318.7177 - val_r2_keras: -1.7824e+09\n",
            "Epoch 19/100\n",
            "3873/3873 - 41s - loss: 259.5459 - mae: 12.0693 - mse: 259.5459 - r2_keras: -1.7579e+05 - val_loss: 268.9424 - val_mae: 12.3894 - val_mse: 268.9424 - val_r2_keras: -7.8925e+09\n",
            "Epoch 20/100\n",
            "3873/3873 - 41s - loss: 253.4505 - mae: 11.8999 - mse: 253.4505 - r2_keras: -6.8112e+05 - val_loss: 265.3044 - val_mae: 12.8262 - val_mse: 265.3044 - val_r2_keras: -1.2552e+10\n",
            "Epoch 21/100\n",
            "3873/3873 - 41s - loss: 246.7017 - mae: 11.7226 - mse: 246.7017 - r2_keras: -6.5955e+04 - val_loss: 406.2158 - val_mae: 14.3552 - val_mse: 406.2158 - val_r2_keras: -6.0577e+08\n",
            "Epoch 22/100\n",
            "3873/3873 - 41s - loss: 243.1245 - mae: 11.6549 - mse: 243.1245 - r2_keras: -5.0460e+05 - val_loss: 214.3377 - val_mae: 11.5467 - val_mse: 214.3377 - val_r2_keras: -7.5308e+09\n",
            "Epoch 23/100\n",
            "3873/3873 - 41s - loss: 238.2514 - mae: 11.5010 - mse: 238.2514 - r2_keras: -4.2520e+05 - val_loss: 220.4813 - val_mae: 11.2773 - val_mse: 220.4813 - val_r2_keras: -5.9678e+09\n",
            "Epoch 24/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mnl4JWxypQj"
      },
      "source": [
        "* 3c. Training Data Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGW9N2X3yq3f"
      },
      "source": [
        "# Metrics from fitting the model on our Training Data\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('\\nMAE: {}'.format(scores[1]))\n",
        "print('\\nMSE: {}'.format(scores[2]))\n",
        "print('\\nR^2: {}'.format(scores[3]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3gFau6KysG7"
      },
      "source": [
        "* 3d. Preparation of Test Data to feed into our model for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WfA4r2YytdI"
      },
      "source": [
        "# Preparing Test Data to be fed into our model for evaluation\n",
        "seq_array_test_last = [test2_df[test2_df['id']==id][sequence_cols].values[-sequence_length:] \n",
        "                       for id in test2_df['id'].unique() if len(test2_df[test2_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY9_xAbCy11i"
      },
      "source": [
        "# Preparing Test Data to be fed into our model for evaluation\n",
        "y_mask = [len(test2_df[test2_df['id']==id]) >= sequence_length for id in test2_df['id'].unique()]\n",
        "label_array_test_last = test2_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zt88zYUzEG3"
      },
      "source": [
        "* 3e. Testing Data Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn9Xj0WBzFj_"
      },
      "source": [
        "# Test Data Metrics after running our model\n",
        "scores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose = 1, batch_size = 200)\n",
        "print('\\nMAE: {}'.format(scores_test[1]))\n",
        "print('\\nMSE: {}'.format(scores_test[2]))\n",
        "print('\\nRMSE:')\n",
        "print(math.sqrt(scores_test[2]))\n",
        "print('\\nR2: {}'.format(scores_test[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN03I9zLzLeM"
      },
      "source": [
        "## Predicted Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SG4pL4uzJig"
      },
      "source": [
        "# To obtain our model's prediction on the Test Data\n",
        "scores_test = model.predict(seq_array_test_last)\n",
        "scores_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jvAVOZzQFM"
      },
      "source": [
        "## SHAP Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PihOTkNwzVai"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXTSncqdzV9O"
      },
      "source": [
        "## Start Modelling for FD003 (Dataset 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFhn_LXvzfmX"
      },
      "source": [
        "seq_gen = (list(gen_sequence(train3_df[train3_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train3_df['id'].unique())\n",
        "\n",
        "# Generate a sequence with the gen_sequence function to get the 3-Dimensional Form \n",
        "# Convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuHVsjSszjdf"
      },
      "source": [
        "# Function to generate the RUL labels \n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# Generate the labels using the gen_labels function\n",
        "label_gen = [gen_labels(train3_df[train3_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train3_df['id'].unique()]\n",
        "# Convert to numpy array\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXtC97pRzogK"
      },
      "source": [
        "## 3. Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfsJ0dBDzpyz"
      },
      "source": [
        "* 3a. Defining R2 to be used as an evaluation metric\n",
        "> R2 is a statistical measures which gives an indication of closeness of fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNvfBSupzrTi"
      },
      "source": [
        "# Defining R2 to be used as an evaluation metric in the Deep Learning Model Evaluation Metrics\n",
        "def r2_keras(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDVkTCVfzs_0"
      },
      "source": [
        "## Deep Learning Architecture and Topology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jv1RMDszuzD"
      },
      "source": [
        "* 3b. Deep Learning Architecture consists of:\n",
        "<br> > Sequential Model\n",
        "<br> > Bi-LSTM Layer\n",
        "<br> > LSTM Layer\n",
        "<br> > Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lUdpZMJzwYo"
      },
      "source": [
        "# Building the Deep Learning Architecture\n",
        "\n",
        "# Defining the features to be fed into the input of the layers in the Deep Learning Model\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "# Sequential Model\n",
        "model = Sequential()\n",
        "# Add a Bi-LSTM Layer\n",
        "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(sequence_length, nb_features)))\n",
        "# Add a Dropout Layer after the Bi-LSTM Layer to minimize overfitting  \n",
        "model.add(Dropout(0.2))\n",
        "# Add a LSTM Layer\n",
        "model.add(LSTM(units=50,return_sequences=False, activation = 'tanh'))\n",
        "# Add a Dropout Layer after the LSTM Layer to minimize overfitting  \n",
        "model.add(Dropout(0.2))\n",
        "# Dense Layer\n",
        "model.add(Dense(units=nb_out))\n",
        "# Activation Function \n",
        "model.add(Activation(\"linear\"))\n",
        "# Compile model, set metrics for evaluation\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae','mse',r2_keras])\n",
        "# For model observation\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the Deep Learning network on our training data, early stopping is also applied\n",
        "history = model.fit(seq_array, label_array, epochs=100, batch_size=10, validation_split=0.05, verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ79EewxzzEX"
      },
      "source": [
        "* 3c. Training Data Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2RZhPRrz0Yn"
      },
      "source": [
        "# Metrics from fitting the model on our Training Data\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('\\nMAE: {}'.format(scores[1]))\n",
        "print('\\nMSE: {}'.format(scores[2]))\n",
        "print('\\nR^2: {}'.format(scores[3]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3USbp38Ez15w"
      },
      "source": [
        "* 3d. Preparation of Test Data to feed into our model for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGDqudEVz3VQ"
      },
      "source": [
        "# Preparing Test Data to be fed into our model for evaluation\n",
        "seq_array_test_last = [test3_df[test3_df['id']==id][sequence_cols].values[-sequence_length:] \n",
        "                       for id in test3_df['id'].unique() if len(test3_df[test3_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwG2a-Q6z9uY"
      },
      "source": [
        "# Preparing Test Data to be fed into our model for evaluation\n",
        "y_mask = [len(test3_df[test3_df['id']==id]) >= sequence_length for id in test3_df['id'].unique()]\n",
        "label_array_test_last = test3_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DWW5RC5z-Z3"
      },
      "source": [
        "* 3e. Testing Data Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ2KcIInz_3P"
      },
      "source": [
        "# Test Data Metrics after running our model\n",
        "scores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose = 1, batch_size = 200)\n",
        "print('\\nMAE: {}'.format(scores_test[1]))\n",
        "print('\\nMSE: {}'.format(scores_test[2]))\n",
        "print('\\nRMSE:')\n",
        "print(math.sqrt(scores_test[2]))\n",
        "print('\\nR2: {}'.format(scores_test[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnq9dn-V0B2g"
      },
      "source": [
        "# Predicted Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qMzwj61z4im"
      },
      "source": [
        "# To obtain our model's prediction on the Test Data\n",
        "scores_test = model.predict(seq_array_test_last)\n",
        "scores_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ma_DXi0F4V"
      },
      "source": [
        "## SHAP Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAAh8Zhl0E5g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCXv14fU0IIo"
      },
      "source": [
        "## Start Modelling for FD004 (Dataset 4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_H6uvb00LGO"
      },
      "source": [
        "seq_gen = (list(gen_sequence(train4_df[train4_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train4_df['id'].unique())\n",
        "\n",
        "# Generate a sequence with the gen_sequence function to get the 3-Dimensional Form \n",
        "# Convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbRfDdR80Rau"
      },
      "source": [
        "# Function to generate the RUL labels \n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# Generate the labels using the gen_labels function\n",
        "label_gen = [gen_labels(train4_df[train4_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train4_df['id'].unique()]\n",
        "# Convert to numpy array\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cxyzN290VYA"
      },
      "source": [
        "## 3. Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIjF58vk0Xdc"
      },
      "source": [
        "* 3a. Defining R2 to be used as an evaluation metric\n",
        "> R2 is a statistical measures which gives an indication of closeness of fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II40KUll0ZDn"
      },
      "source": [
        "# Defining R2 to be used as an evaluation metric in the Deep Learning Model Evaluation Metrics\n",
        "def r2_keras(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz2jmlrW0bV7"
      },
      "source": [
        "## Deep Learning Architecture and Topology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW6eA7SB0c0N"
      },
      "source": [
        "* 3b. Deep Learning Architecture consists of:\n",
        "<br> > Sequential Model\n",
        "<br> > Bi-LSTM Layer\n",
        "<br> > LSTM Layer\n",
        "<br> > Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_FlKxXL0exT"
      },
      "source": [
        "# Building the Deep Learning Architecture\n",
        "\n",
        "# Defining the features to be fed into the input of the layers in the Deep Learning Model\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "# Sequential Model\n",
        "model = Sequential()\n",
        "# Add a Bi-LSTM Layer\n",
        "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(sequence_length, nb_features)))\n",
        "# Add a Dropout Layer after the Bi-LSTM Layer to minimize overfitting  \n",
        "model.add(Dropout(0.2))\n",
        "# Add a LSTM Layer\n",
        "model.add(LSTM(units=50,return_sequences=False, activation = 'tanh'))\n",
        "# Add a Dropout Layer after the LSTM Layer to minimize overfitting  \n",
        "model.add(Dropout(0.2))\n",
        "# Dense Layer\n",
        "model.add(Dense(units=nb_out))\n",
        "# Activation Function \n",
        "model.add(Activation(\"linear\"))\n",
        "# Compile model, set metrics for evaluation\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae','mse',r2_keras])\n",
        "# For model observation\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the Deep Learning network on our training data, early stopping is also applied\n",
        "history = model.fit(seq_array, label_array, epochs=100, batch_size=10, validation_split=0.05, verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rJ47EL10hh3"
      },
      "source": [
        "* 3c. Training Data Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw-KKFQC0gwk"
      },
      "source": [
        "# Metrics from fitting the model on our Training Data\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('\\nMAE: {}'.format(scores[1]))\n",
        "print('\\nMSE: {}'.format(scores[2]))\n",
        "print('\\nR^2: {}'.format(scores[3]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za8oUAX20k39"
      },
      "source": [
        "* 3d. Preparation of Test Data to feed into our model for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxOU6VxV0mra"
      },
      "source": [
        "# Preparing Test Data to be fed into our model for evaluation\n",
        "seq_array_test_last = [test4_df[test4_df['id']==id][sequence_cols].values[-sequence_length:] \n",
        "                       for id in test4_df['id'].unique() if len(test4_df[test4_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo04khkQ0uU6"
      },
      "source": [
        "# Preparing Test Data to be fed into our model for evaluation\n",
        "y_mask = [len(test4_df[test4_df['id']==id]) >= sequence_length for id in test4_df['id'].unique()]\n",
        "label_array_test_last = test4_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cahOfAN0k6R"
      },
      "source": [
        "* 3e. Testing Data Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUfpicWf01xK"
      },
      "source": [
        "# Test Data Metrics after running our model\n",
        "scores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose = 1, batch_size = 200)\n",
        "print('\\nMAE: {}'.format(scores_test[1]))\n",
        "print('\\nMSE: {}'.format(scores_test[2]))\n",
        "print('\\nRMSE:')\n",
        "print(math.sqrt(scores_test[2]))\n",
        "print('\\nR2: {}'.format(scores_test[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUdc_jLU03kJ"
      },
      "source": [
        "# Predicted Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHrVok6V0404"
      },
      "source": [
        "# To obtain our model's prediction on the Test Data\n",
        "scores_test = model.predict(seq_array_test_last)\n",
        "scores_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIRRQUs207Hx"
      },
      "source": [
        "## SHAP Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iszUJNYb0_aU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}