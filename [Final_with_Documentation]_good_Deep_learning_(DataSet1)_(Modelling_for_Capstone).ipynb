{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR-TduwL5m4Z"
   },
   "source": [
    "## 1. Import Relevant Libraries and Reading of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thz_jf7D2Gky"
   },
   "source": [
    "* 1a. Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "H2c_xCew5rkx"
   },
   "outputs": [],
   "source": [
    "# Import Required Libaries\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, TimeDistributed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucybXgp72VqS"
   },
   "source": [
    "* 1b. Reading in the Data:\n",
    "<br><i> > Training Data\n",
    "<br> > Testing Data\n",
    "<br> > True Labels (RUL) for the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "ve7h1KnM5zeO"
   },
   "outputs": [],
   "source": [
    "# Reading in the data (txt files)\n",
    "\n",
    "# Train Data\n",
    "train_df = pd.read_csv(\"/content/train_FD001.txt\", sep=\" \", header=None)\n",
    "# Test Data\n",
    "test_df = pd.read_csv(\"/content/test_FD001.txt\", sep=\" \", header=None)\n",
    "# True RUL Data for the Test Set\n",
    "truth_df = pd.read_csv(\"/content/RUL_FD001.txt\", sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghG_80mg5t-E"
   },
   "source": [
    "* 1c. We set a seed for Reproducibility of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "P1iFWcBtAk_D"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg53D_xCAnpY"
   },
   "source": [
    "* 1d. We create a path to save the Deep Learning model output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "TOEy7RJ3AlcL"
   },
   "outputs": [],
   "source": [
    "# define a path to save model\n",
    "model_path = '/content/Output/regression_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-QwvxhLA85I"
   },
   "source": [
    "## 2. Data Preprocessing\n",
    "- Drop the last 2 columns which consists of all null values\n",
    "- Rename columns according to the Engine ID, Cycle, the 3 respective Operating Settings and the 21 Operating Sensors.\n",
    "- Derive the RUL from the train data. This is because the RUL for the train data is not explicity provided.\n",
    "- MinMax Normalisation to transform our features' values to a value between 0 and 1; 0 represents the minimum output value of the sensor value, while 1 represents the maximum output value of the sensor value.\n",
    "- Clipping the upper limit of the RUL of aircrafts to mimic a more accurate degradation pattern of the aircraft engine with increasing usage.\n",
    "- Transform the data into a form (3-Dimensional Form) that can be fed into the deep learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swkjjtsw2z62"
   },
   "source": [
    "* 2a. Drop the last 2 columns for Training, Testing and True Labels Dataframes\n",
    "> This is because the last 2 columns consists of all null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "mwohCRTRO3iA"
   },
   "outputs": [],
   "source": [
    "# Drop the last 2 columns for Train, Test and True RUL Dataframes\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX8ustEm25Qx"
   },
   "source": [
    "* 2b. Rename Columns in this form: \n",
    "<br><i>-> Engine ID\n",
    "<br>-> 3 Operating Settings\n",
    "<br>-> 21 Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "QE6dyXzp23G_"
   },
   "outputs": [],
   "source": [
    "# Rename columns into readable forms, namely, by: \n",
    "# [Engine ID, Operational Settings, Sensors]\n",
    "\n",
    "\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQI_x7RO3OdL"
   },
   "source": [
    "* 2c. Derive RUL for the Training Data\n",
    "<br><i> This is because the RUL for the Training Data is not explicity provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "M60YfXIzP5sH"
   },
   "outputs": [],
   "source": [
    "# Derive the RUL for the Train Data\n",
    "def extract_rul(data, factor = 0):\n",
    "\n",
    "    # Get the total number of cycles for each unit, i.e. Each Engine ID\n",
    "    rul = pd.DataFrame(data.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "\n",
    "    # Merge the maximum cycle into the original dataframe\n",
    "    data = data.merge(rul, on=['id'], how='left')\n",
    "\n",
    "    # Actual calculation of RUL\n",
    "    data['RUL'] = data['max'] - data['cycle']\n",
    "\n",
    "    # Drop the 'max' column, which is now redundant\n",
    "    data.drop(columns=['max'], axis = 1, inplace = True)\n",
    "  \n",
    "    return data[data['cycle'] > factor]\n",
    "\n",
    "# Apply the function on the training data\n",
    "train_df = extract_rul(train_df, factor = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--EaD7hR3XI_"
   },
   "source": [
    "* 2d. Clipping the maximum RUL to aircraft engines for Training Data \n",
    "> This is to more accurately portray the degradation pattern of aircraft engines after a certain period of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "-uP-P1xSRIXw"
   },
   "outputs": [],
   "source": [
    "# Clipping the maximum RUL to aircraft engines (for train data)\n",
    "train_df['RUL'] = train_df['RUL'].clip(upper=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAlEFLfR3i4N"
   },
   "source": [
    "* 2e. MinMax Normalisation of Training Data\n",
    "> Transforms our features' values to a value between 0 and 1; 0 represents the minimum output value of the sensor value, while 1 represents the maximum output value of the sensor value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "esDY2KYs3Q8b"
   },
   "outputs": [],
   "source": [
    "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for train set\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "\n",
    "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for test set\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPs4Mpfj5Y0O"
   },
   "source": [
    "* 2f. Processing of Test Data with the True RUL Labels of the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "p8wBrqgxSFlh"
   },
   "outputs": [],
   "source": [
    "# Generate labels (the RUL) for the Test Data using the dataset containing the true RUL for the Test Data.\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "# rename column in Truth Dataframe\n",
    "truth_df.columns = ['rul_init']\n",
    "# assign the RUL to the Engine IDs by adding a column in Truth Dataframe\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "# assign RUL to a newly-named column, max\n",
    "truth_df['max'] = rul['max'] + truth_df['rul_init']\n",
    "truth_df.drop('rul_init', axis=1, inplace=True)\n",
    "\n",
    "# Merge Truth Dataframe with Test Dataframe to put the actual RUL together with the data in Test Dataframe\n",
    "# Merge based on Engine ID\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "# Final RUL matched to Engine ID\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "\n",
    "# Clipping the maximum RUL to aircraft engines.\n",
    "test_df['RUL'] = test_df['RUL'].clip(upper=125)\n",
    "test_df.drop('max', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl2HMyD65hb9"
   },
   "source": [
    "* 2g: Transform data into a form that can be fed into the Deep Learning Model as input.\n",
    "> [samples, time steps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "dg1KaBYi2VB7"
   },
   "outputs": [],
   "source": [
    "# Transforming data into a form that can be fed in to the Deep Learning Model \n",
    "# The Form is a(3 Dimensional Form): (samples, time steps, features) \n",
    "\n",
    "# Assign sequence length of 50 \n",
    "sequence_length = 50\n",
    "# Generate the sequence  \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "yye0wym13it6"
   },
   "outputs": [],
   "source": [
    "# Feature columns consists of the Operational Settings and Sensor Columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# Generate a sequence with the gen_sequence function to get the 3-Dimensional Form \n",
    "# Convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6r-9he23t06",
    "outputId": "db4fc574-6c84-4fe8-ac6a-47f0c6face83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15631, 1)"
      ]
     },
     "execution_count": 154,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the RUL labels \n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# Generate the labels using the gen_labels function\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
    "             for id in train_df['id'].unique()]\n",
    "# Convert to numpy array\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60gq0BSyMFxX"
   },
   "source": [
    "## 3. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRpsXr_16CT6"
   },
   "source": [
    "* 3a. Defining R2 to be used as an evaluation metric\n",
    "> R2 is a statistical measures which gives an indication of closeness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "Qf1ssR9A4Ec2"
   },
   "outputs": [],
   "source": [
    "# Defining R2 to be used as an evaluation metric in the Deep Learning Model Evaluation Metrics\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuPlcuGy6fus"
   },
   "source": [
    "* 3b. Defining Adjusted R2 to be used as an evaluation metric\n",
    "> The adjusted R-squared is a modified version of R-squared that accounts for predictors that are not significant in a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zczPBC9V6krZ"
   },
   "outputs": [],
   "source": [
    "# Adjusted R2 score\n",
    "def adjusted_r2(r2, p, n):\n",
    "  numerator = (1-r2)*(n-1)\n",
    "  denom = n - p - 1\n",
    "  result = 1 - (numerator/denom)\n",
    "  return result\n",
    "\n",
    "print('Adjusted R2:')\n",
    "print(adjusted_r2(scores[2], nb_features, len(label_array)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAwRm9W9UiLB"
   },
   "source": [
    "## Deep Learning Architecture and Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVo_bDmL7b3y"
   },
   "source": [
    "* 3c. Deep Learning Architecture consists of:\n",
    "<br> > Sequential Model\n",
    "<br> > Bi-LSTM Layer\n",
    "<br> > LSTM Layer\n",
    "<br> > Dropout Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QiNrt2VugfV",
    "outputId": "3391c89c-eceb-4bb7-cbc4-55ed7fc56c1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_9 (Bidirection (None, 50, 40)            7360      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 50, 40)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 50)                18200     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 25,611\n",
      "Trainable params: 25,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1485/1485 - 17s - loss: 3097.5068 - mse: 3097.5068 - r2_keras: -1.2908e+00 - val_loss: 1728.9891 - val_mse: 1728.9891 - val_r2_keras: -5.4805e+10\n",
      "Epoch 2/5\n",
      "1485/1485 - 13s - loss: 1743.5453 - mse: 1743.5453 - r2_keras: -1.6592e-01 - val_loss: 1706.7083 - val_mse: 1706.7083 - val_r2_keras: -4.9675e+10\n",
      "Epoch 3/5\n",
      "1485/1485 - 13s - loss: 737.8798 - mse: 737.8798 - r2_keras: 0.4822 - val_loss: 372.7946 - val_mse: 372.7946 - val_r2_keras: -8.0248e+09\n",
      "Epoch 4/5\n",
      "1485/1485 - 13s - loss: 312.6321 - mse: 312.6321 - r2_keras: 0.7633 - val_loss: 229.6667 - val_mse: 229.6667 - val_r2_keras: -3.4589e+09\n",
      "Epoch 5/5\n",
      "1485/1485 - 13s - loss: 250.5266 - mse: 250.5266 - r2_keras: 0.8120 - val_loss: 172.8846 - val_mse: 172.8846 - val_r2_keras: -2.6441e+09\n"
     ]
    }
   ],
   "source": [
    "# Building the Deep Learning Architecture\n",
    "\n",
    "# Defining the features to be fed into the input of the layers in the Deep Learning Model\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "# Sequential Model\n",
    "model = Sequential()\n",
    "# Add a Bi-LSTM Layer\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(sequence_length, nb_features)))\n",
    "# Add a Dropout Layer after the Bi-LSTM Layer to minimize overfitting  \n",
    "model.add(Dropout(0.2))\n",
    "# Add a LSTM Layer\n",
    "model.add(LSTM(units=50,return_sequences=False, activation = 'tanh'))\n",
    "# Add a Dropout Layer after the LSTM Layer to minimize overfitting  \n",
    "model.add(Dropout(0.2))\n",
    "# Dense Layer\n",
    "model.add(Dense(units=nb_out))\n",
    "# Activation Function \n",
    "model.add(Activation(\"linear\"))\n",
    "# Compile model, set metrics for evaluation\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mse',r2_keras])\n",
    "# For model observation\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the Deep Learning network on our training data\n",
    "history = model.fit(seq_array, label_array, epochs=5, batch_size=10, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIdo_0JC8Ds8"
   },
   "source": [
    "* 3d. Training Data Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NxCoSBIU6KIS",
    "outputId": "16b0c3a5-2373-45e9-f119-475064d4e331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 5ms/step - loss: 166.2797 - mse: 166.2797 - r2_keras: 0.8655\n",
      "\n",
      "MSE: 166.27967834472656\n",
      "\n",
      "R^2: 0.8654634356498718\n"
     ]
    }
   ],
   "source": [
    "# Metrics from fitting the model on our Training Data\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('\\nMSE: {}'.format(scores[1]))\n",
    "print('\\nR^2: {}'.format(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kX9Si6OrU7js",
    "outputId": "ce428729-6c0a-4fe1-f765-dde1d39b8a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R2:\n",
      "0.8652479012628963\n"
     ]
    }
   ],
   "source": [
    "# Adjusted R2 score\n",
    "def adjusted_r2(r2, p, n):\n",
    "  numerator = (1-r2)*(n-1)\n",
    "  denom = n - p - 1\n",
    "  result = 1 - (numerator/denom)\n",
    "  return result\n",
    "\n",
    "print('Adjusted R2:')\n",
    "print(adjusted_r2(scores[2], nb_features, len(label_array)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa18svH-8H4X"
   },
   "source": [
    "* 3e. Preparation of Test Data to feed into our model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "zdTs1FAP6UXP"
   },
   "outputs": [],
   "source": [
    "# Preparing Test Data to be fed into our model for evaluation\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "I_1VG4pA7ISQ"
   },
   "outputs": [],
   "source": [
    "# Preparing Test Data to be fed into our model for evaluation\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsIoTFZ08PnA"
   },
   "source": [
    "* 3f. Testing Data Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkPBwLRy7Jat",
    "outputId": "94c34271-0567-46da-bcc9-5276f0ad24e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 176.8562 - mse: 176.8562 - r2_keras: 0.8892\n",
      "\n",
      "MSE: 176.8561553955078\n",
      "\n",
      "R^2: 0.889172375202179\n",
      "\n",
      "RMSE:\n",
      "13.298727585581554\n"
     ]
    }
   ],
   "source": [
    "# Test Data Metrics after running our model\n",
    "scores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose = 1, batch_size = 200)\n",
    "print('\\nMSE: {}'.format(scores_test[1]))\n",
    "print('\\nR^2: {}'.format(scores_test[2]))\n",
    "print('\\nRMSE:')\n",
    "print(math.sqrt(scores_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xig-6TmG8U4i"
   },
   "source": [
    "# Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALDR7gbrI6Ve",
    "outputId": "d9c4f5a3-44eb-4e6f-ae97-9bb3d7f84892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67.94558  ],\n",
       "       [ 95.82517  ],\n",
       "       [102.17069  ],\n",
       "       [113.70323  ],\n",
       "       [111.678925 ],\n",
       "       [101.63217  ],\n",
       "       [115.98318  ],\n",
       "       [ 80.206314 ],\n",
       "       [101.06471  ],\n",
       "       [ 97.11608  ],\n",
       "       [ 90.80011  ],\n",
       "       [102.6706   ],\n",
       "       [ 94.159325 ],\n",
       "       [ 58.36712  ],\n",
       "       [ 31.211872 ],\n",
       "       [ 87.43528  ],\n",
       "       [  7.7870865],\n",
       "       [ 77.095566 ],\n",
       "       [113.072586 ],\n",
       "       [ 18.947838 ],\n",
       "       [116.63224  ],\n",
       "       [ 70.306786 ],\n",
       "       [113.351494 ],\n",
       "       [ 96.63779  ],\n",
       "       [ 94.49725  ],\n",
       "       [  3.8548162],\n",
       "       [ 46.69287  ],\n",
       "       [107.671646 ],\n",
       "       [  3.5280566],\n",
       "       [  2.298322 ],\n",
       "       [ 10.054273 ],\n",
       "       [ 28.816008 ],\n",
       "       [ 54.738876 ],\n",
       "       [ 18.172281 ],\n",
       "       [ 23.952404 ],\n",
       "       [  8.486936 ],\n",
       "       [ 78.33343  ],\n",
       "       [117.72732  ],\n",
       "       [ 87.320885 ],\n",
       "       [ 43.000294 ],\n",
       "       [113.288895 ],\n",
       "       [114.36866  ],\n",
       "       [ 19.53542  ],\n",
       "       [ 77.72872  ],\n",
       "       [107.537575 ],\n",
       "       [ 30.95918  ],\n",
       "       [ 31.469212 ],\n",
       "       [109.33628  ],\n",
       "       [119.2756   ],\n",
       "       [  5.4339423],\n",
       "       [104.158    ],\n",
       "       [ 42.415367 ],\n",
       "       [105.43064  ],\n",
       "       [110.89972  ],\n",
       "       [ 19.063364 ],\n",
       "       [ 43.15064  ],\n",
       "       [ 77.2508   ],\n",
       "       [ 24.209496 ],\n",
       "       [118.296326 ],\n",
       "       [  9.832399 ],\n",
       "       [119.03829  ],\n",
       "       [  3.6472816],\n",
       "       [118.55998  ],\n",
       "       [101.467705 ],\n",
       "       [107.58867  ],\n",
       "       [ 62.966343 ],\n",
       "       [ 99.876015 ],\n",
       "       [114.710655 ],\n",
       "       [ 99.712326 ],\n",
       "       [  3.493465 ],\n",
       "       [ 37.403374 ],\n",
       "       [120.222015 ],\n",
       "       [ 79.3529   ],\n",
       "       [ 96.06554  ],\n",
       "       [  2.498384 ],\n",
       "       [  5.935234 ],\n",
       "       [117.606186 ],\n",
       "       [ 66.95366  ],\n",
       "       [114.30311  ],\n",
       "       [118.01746  ],\n",
       "       [104.78799  ],\n",
       "       [113.6885   ],\n",
       "       [ 20.252182 ],\n",
       "       [ 19.206297 ],\n",
       "       [ 10.625554 ],\n",
       "       [ 44.358112 ],\n",
       "       [ 63.359917 ],\n",
       "       [112.16675  ],\n",
       "       [103.6076   ],\n",
       "       [105.26006  ],\n",
       "       [ 95.81991  ],\n",
       "       [114.22959  ],\n",
       "       [ 18.69003  ]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To obtain our model's prediction on the Test Data\n",
    "scores_test = model.predict(seq_array_test_last)\n",
    "scores_test\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[Final with Documentation] good Deep-learning (DataSet1)  (Modelling for Capstone) ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
