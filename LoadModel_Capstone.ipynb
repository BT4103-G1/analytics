{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LoadModel_Capstone.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR-TduwL5m4Z"
      },
      "source": [
        "## 1. Import Relevant Libraries and Reading In of User's Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thz_jf7D2Gky"
      },
      "source": [
        "* 1a. Importing Relevant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2c_xCew5rkx"
      },
      "source": [
        "# Import Required Libaries\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers.core import Activation\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, TimeDistributed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from keras.layers import Bidirectional\n",
        "import math \n",
        "import gdown\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucybXgp72VqS"
      },
      "source": [
        "* 1b. Reading in User's Data:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffKuwMJ76e2F"
      },
      "source": [
        "## ** To User: Please load your dataset here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GolPRt8gWD0"
      },
      "source": [
        "> Insert your Dataset file path in the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgW3AVjvglv3"
      },
      "source": [
        "# FD001 (Dataset 1)\n",
        "\n",
        "## Note: here, we are assuming that the training set of FD001 is the user's dataset\n",
        "\n",
        "train1 = \"/content/train_FD001.txt\""
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS_Cu82ShnD-"
      },
      "source": [
        "> 1c. Data is being read in ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve7h1KnM5zeO"
      },
      "source": [
        "# For FD001\n",
        "# Train Data \n",
        "train1_df = pd.read_csv(train1, sep=\" \", header=None)\n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg53D_xCAnpY"
      },
      "source": [
        "* 1d. We load the trained Deep Learning Model here: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd7GNY-B-44b"
      },
      "source": [
        "# import model\n",
        "# This URL contains the trained model:\n",
        "url = 'https://drive.google.com/uc?id=1lCDBubTqPwyDo0j3Kt0XrAkdTltZrlJz'"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fwuy6RH6q1_"
      },
      "source": [
        "## ** To User: Please enter your desired output file path in the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBNTC4ZchvWZ"
      },
      "source": [
        "> Import the model to your desired output file path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "Fdh6u3UB4oHn",
        "outputId": "357bebb7-2173-4e98-9c6a-7edd67d87d0b"
      },
      "source": [
        "# Insert your desired output file path here:\n",
        "output = '/content/deep_learning_model.h5'\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lCDBubTqPwyDo0j3Kt0XrAkdTltZrlJz\n",
            "To: /content/deep_learning_model.h5\n",
            "100%|██████████| 257k/257k [00:00<00:00, 8.60MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/deep_learning_model.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDWYDJCP2Nkr"
      },
      "source": [
        "# Defining R2 to be used as an evaluation metric in the Deep Learning Model Evaluation Metrics\n",
        "def r2_keras(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "\n",
        "dependencies = {\n",
        "    'r2_keras': r2_keras\n",
        "}\n",
        "\n",
        "model = keras.models.load_model(output, custom_objects=dependencies)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-QwvxhLA85I"
      },
      "source": [
        "## 2. Data Preprocessing (Assuming User's Dataset is in the same format as the providedcNASA CMAPSS Dataset)\n",
        "- Dealing with Missing Values: Drop the last 2 columns, because they consist of all null values\n",
        "- Rename columns according to the Engine ID, Cycle, the 3 respective Operating Settings and the 21 Operating Sensors. This aids in better readability and interpretability of the dataset.\n",
        "- Derive RUL labels from the train data. This is because the RUL for the train data is not explicity provided.\n",
        "- MinMax Normalisation to transform our features' values to a value between 0 and 1; For every feature, the minimum value of the feature would be transformed to a value of 0, and the maximum value of the feature would be transformed to a value of 1. \n",
        "- Contextual handling of RUL: Clipping the upper limit of the RUL of aircrafts to mimic a more accurate degradation pattern of the aircraft engine with increasing usage.\n",
        "- Transform the input data into a form (3-Dimensional Form) that can be fed into the deep learning model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swkjjtsw2z62"
      },
      "source": [
        "* 2a. Dealing with Missing Values: Drop the last 2 columns for Training, Testing and True Labels Dataframes\n",
        "> This is because the last 2 columns consists of all null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwohCRTRO3iA"
      },
      "source": [
        "# Drop the last 2 columns for Train, Test and True RUL Dataframes\n",
        "\n",
        "# FD001\n",
        "train1_df.drop(train1_df.columns[[26, 27]], axis=1, inplace=True)\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX8ustEm25Qx"
      },
      "source": [
        "* 2b. Rename Columns in this form for better readability and interpretability of the dataset: \n",
        "<br><i>-> Engine ID\n",
        "<br>-> 3 Operating Settings\n",
        "<br>-> 21 Sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE6dyXzp23G_"
      },
      "source": [
        "# Rename columns into readable forms, namely, by: \n",
        "# [Engine ID, Operational Settings, Sensors]\n",
        "\n",
        "# FD001\n",
        "train1_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "train1_df = train1_df.sort_values(['id','cycle'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQI_x7RO3OdL"
      },
      "source": [
        "* 2c. Derive RUL labels for the Training Data\n",
        "<br><i> This is because the RUL for the Training Data is not explicity provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uHCLDtXj4XC"
      },
      "source": [
        "Since this dataset is a simulated run-to-failure dataset, the last row for each Engine ID would represent the last cycle of the engine, indicating aircraft failure had already occurred. Hence, since we already know the complete life cycle length of Engines, we employed the following calculations for RUL derivation:\n",
        "> * RUL (of each row/instance of each Engine ID) = Maximum cycle length for each Engine ID - current cycle length (at each row/instance of each Engine ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M60YfXIzP5sH"
      },
      "source": [
        "# Function to derive the RUL labels for the Train Data\n",
        "def extract_rul(data, factor = 0):\n",
        "\n",
        "    # Get the total number of cycles for each unit, i.e. Each Engine ID\n",
        "    # The last row (i.e. maximum value) for each Engine ID would represent the last cycle of the engine\n",
        "    rul = pd.DataFrame(data.groupby('id')['cycle'].max()).reset_index()\n",
        "    rul.columns = ['id', 'max']\n",
        "\n",
        "    # Merge the maximum cycle into the original dataframe\n",
        "    data = data.merge(rul, on=['id'], how='left')\n",
        "\n",
        "    # Actual calculation of RUL (based on the maximum value of the RUL for each Engine)\n",
        "    data['RUL'] = data['max'] - data['cycle']\n",
        "\n",
        "    # Drop the 'max' column, which now does not add any value\n",
        "    data.drop(columns=['max'], axis = 1, inplace = True)\n",
        "  \n",
        "    return data[data['cycle'] > factor]\n",
        "\n",
        "# Apply the function on the data:\n",
        "# FD001\n",
        "train1_df = extract_rul(train1_df, factor = 0)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--EaD7hR3XI_"
      },
      "source": [
        "* 2d. Contextual Handling of RUL: Clipping the maximum RUL to aircraft engines for Training Data \n",
        "> This is to more accurately portray the degradation pattern of aircraft engines after a certain period of usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uP-P1xSRIXw"
      },
      "source": [
        "# Clipping the maximum RUL to aircraft engines (for train data):\n",
        "\n",
        "# FD001\n",
        "train1_df['RUL'] = train1_df['RUL'].clip(upper=125)\n",
        "\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAlEFLfR3i4N"
      },
      "source": [
        "* 2e. MinMax Normalisation of Training Data\n",
        "> * Transforms our features' values to a value between 0 and 1; \n",
        "> * The minimum value of the feature would be transformed to a value of 0, and the maximum value of the feature would be transformed to a value of 1. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esDY2KYs3Q8b"
      },
      "source": [
        "# FD001\n",
        "# MinMax normalization of Operational Settings and Sensor Values (from 0 to 1) for train set\n",
        "train1_df['cycle_norm'] = train1_df['cycle']\n",
        "cols_normalize = train1_df.columns.difference(['id','cycle','RUL'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train1_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train1_df.index)\n",
        "join_df = train1_df[train1_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train1_df = join_df.reindex(columns = train1_df.columns)\n",
        "\n"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl2HMyD65hb9"
      },
      "source": [
        "* 2g: Transform data into a form that can be fed into the Deep Learning Model as input.\n",
        "> The form is: [samples, time steps, features]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg1KaBYi2VB7"
      },
      "source": [
        "# Transforming data into a form that can be fed in to the Deep Learning Model \n",
        "# The Form is a(3 Dimensional Form): (samples, time steps, features) \n",
        "\n",
        "# Assign sequence length of 50 \n",
        "sequence_length = 50\n",
        "# Function to generate the sequence for the data: \n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yye0wym13it6"
      },
      "source": [
        "# Feature columns consists of the Operational Settings and Sensor Columns: \n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzSWWROpUHX"
      },
      "source": [
        "seq_gen = (list(gen_sequence(train1_df[train1_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train1_df['id'].unique())\n",
        "\n",
        "# Generate a sequence with the gen_sequence function to get the 3-Dimensional Form \n",
        "# Convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6r-9he23t06",
        "outputId": "07735367-8817-4856-cae7-8e1c12245da4"
      },
      "source": [
        "# Function to generate the RUL labels \n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# Generate the labels using the gen_labels function\n",
        "label_gen = [gen_labels(train1_df[train1_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train1_df['id'].unique()]\n",
        "# Convert to numpy array\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40759, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60gq0BSyMFxX"
      },
      "source": [
        "## 3. Running the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y4dYjPM2yfc"
      },
      "source": [
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdqUfheZ3L-5"
      },
      "source": [
        "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae','mse',r2_keras])"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIdo_0JC8Ds8"
      },
      "source": [
        "## > Fit data into the model to get the predicted RUL values\n",
        "## * Predicted RUL will be output as a .csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pia2uxoe9bRF"
      },
      "source": [
        "RUL_predicted = model.predict(seq_array)\n",
        "df = pd.DataFrame(RUL_predicted)\n",
        "df.to_csv('predicted_RUL.csv')"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9nkDqHg-QdQ"
      },
      "source": [
        "## SHAP Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q8J9nzIteba",
        "outputId": "87fa38ba-66ff-4a77-8521-6ae98be7997c"
      },
      "source": [
        "%pip install shap\n",
        "import shap"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f4/c5b95cddae15be80f8e58b25edceca105aa83c0b8c86a1edad24a6af80d3/shap-0.39.0.tar.gz (356kB)\n",
            "\r\u001b[K     |█                               | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 25.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 20.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 23.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 23.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 102kB 17.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 143kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153kB 17.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 163kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 174kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 184kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 194kB 17.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 204kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 215kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 225kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 235kB 17.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 245kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 276kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 286kB 17.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 296kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 307kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 317kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 327kB 17.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 337kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 348kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.41.1)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (54.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491620 sha256=e29806ed93c68b453aa7cbbbe75e6a6be784feee27b278294348516e3328c5cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/27/f5/a8ab9da52fd159aae6477b5ede6eaaec69fd130fa0fa59f283\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.39.0 slicer-0.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE2cZ7uh1E3m"
      },
      "source": [
        ""
      ],
      "execution_count": 143,
      "outputs": []
    }
  ]
}